Software Testing
-
- concept:
  - process of evaluating & verifying a software does what it's supposed to do
  - note: testing shows presence, not absence, of bugs
- purposes:
  - prevent bugs; show correctness in my program
  - reduce development costs; prevent regression as code change over time (e.g. when refactoring)
  - improve performance
- types of verification:
  - dynamic verification
    - testing whilst executing code
    - also called run-time checks
  - static verification
    - testing before executing code
    - also called compile-time checks

Software Safety
-
- commonly confused terms:
  - safety: protection from accidental misuse
  - security: protection from intentional misuse
- unsafe examples:
  - reading unitialised memory
  - writing outside array bounds
- memory safety:
  - JS is memory safe, unlike C
    - JS prevents access to uninitialised or unallocated memory
      - done by dynamically checking at runtime
    - C: performance > memory safety
      - hence no bound checking for array accesses
      - and pointers can be dereferenced even if they don't point to allocated memory

Dynamic Verification
-
- concept
  - IRL, bad things happen to programs all the time, e.g. invalid input and unexpected data
  - dynamic test: make program more robust in face of real and inevitable errors in a running program
- unit testing:
  - testing individual components in isolation
  - often just testing a particular function
- testing in the large:
  - expose defects in interfaces & interactions between integrated components or systems
  - commonly black-box tests:
    - testing without knowing internal structures (tester doesn't know how the code works)
    - written by developers or independent testers
    - whereas white-box testing is when the tester understands the code
  - types:
    - module tests: specific module
    - integration tests: integration of modules
    - system tests: entire system

Automating Dynamic Verification
-
- concept:
  - debugging:
    - manually checking output or using `if` statements to print error when comparisons don't match
    - it doesn't scale well; very inefficient as number of tests increase
    - this is not testing
- jest testing:
  - installation: `pm install --save-dev jest`
  - concept:
    - jest runs all `.test.js` files
  - running jest:
    - add `"test": "jest"` to our package.json
    - run `npm run test` in terminal
  - functions (commonly used):
    ```
    test -> wraps a test
    expect -> true to pass
    toStrictEqual -> compare function ouput to expected output
    ```
  - sample structure:
    ```
    import { removeVowels } from './jest_lib';
    
    describe('removeVowels', () => {
      test('deals with no vowels', () => {
        const example1 = removeVowels('bcd');
        const example2 = removeVowels('lkj');
        expect(example1).toEqual('bcd');
        expect(example2).toEqual('lkj');
      });
      test('deals with only vowels', () => {
        expect(removeVowels('aei')).toEqual('');
        expect(removeVowels('oiu')).toEqual('');
      });
      test('deals with starting vowels', () => {
        expect(removeVowels('ant')).toEqual('nt');
        expect(removeVowels('old')).toEqual('ld');
      });
      test('deals with ending vowels', () => {
        expect(removeVowels('bee')).toEqual('b');
        expect(removeVowels('hi')).toEqual('h');
      });
      test('deals with complex words', () => {
        expect(removeVowels('cannot')).toEqual('cnnt');
        expect(removeVowels('delicious')).toEqual('dlcs');
      });
    });
    ```

Design by Contract
-
- concept:
  - we put function header documentations
    - this tells developers the information we work with
    - also expected return values
  - the team then agrees on it (like a contract) before continuing
  - this allows us to assume what checks have been done already (or not done yet) so we can reduce repetitive code

Behaviour
-
- concept:
  - behaviour = what programs do under certain conditions
  - when testing, we're just testing program behaviour
  - however, it's not always possible to know the behaviour of every possible scenario from a spec perspective, due to:
    - not feasible to define every edge case
    - implementation details not critical to spec
- undefined behaviour
  - behaviour not defined
  - often due to case being beyond reasonable scope of expectation
  - that behaviour generally shouldn't occur within reasonable use of system
  - e.g. root of -1
- implementation behaviour
  - behaviour that's defined & considered
  - but it's up to individual implementation to make the decision
  - e.g. text inputs or ASCII or UTF-8 encoding
